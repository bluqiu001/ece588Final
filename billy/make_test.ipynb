{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import (Activation, Concatenate, Conv2D,\n",
    "                                     Conv2DTranspose, Input, LeakyReLU)\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "generator_g_import = tf.keras.models.load_model('generator_g.h5')\n",
    "generator_f_import = tf.keras.models.load_model('generator_f.h5')\n",
    "discriminator_x_import = tf.keras.models.load_model('discriminator_x.h5')\n",
    "discriminator_y_import = tf.keras.models.load_model('discriminator_y.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data, metadata = tfds.load('cycle_gan/monet2photo', with_info=True, as_supervised=True)\n",
    "\n",
    "train_x, train_y, test_x, test_y = data['trainA'], data['trainB'], data['testA'], data['testB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billyluqiu/miniforge3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "epochs = 50\n",
    "\n",
    "LAMBDA = 10\n",
    "\n",
    "img_rows, img_cols, channels = 256, 256, 3\n",
    "weight_initializer = RandomNormal(stddev=0.02)\n",
    "\n",
    "gen_g_optimizer = gen_f_optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "dis_x_optimizer = dis_y_optimizer = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images to [-1, 1] and reshape\n",
    "def preprocess_image(image, _):\n",
    "    return tf.reshape(tf.cast(tf.image.resize(image, (int(img_rows), int(img_cols))), tf.float32) / 127.5 - 1, (1, img_rows, img_cols, channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the normalization onto the dataset\n",
    "train_x = train_x.map(preprocess_image)\n",
    "train_y = train_y.map(preprocess_image)\n",
    "test_x = test_x.map(preprocess_image)\n",
    "test_y = test_y.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (1, 256, 256, 3), types: tf.float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images():\n",
    "    # Sample images\n",
    "    print(test_x)\n",
    "    x = next(iter(test_x.shuffle(1000))).numpy()\n",
    "    y = next(iter(test_y.shuffle(1000))).numpy()\n",
    "    print(x)\n",
    "    # Get predictions for those images\n",
    "    y_hat = generator_g_import.predict(x.reshape((1, img_rows, img_cols, channels)))\n",
    "    x_hat = generator_f_import.predict(y.reshape((1, img_rows, img_cols, channels)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BELOW THIS IS THE TEST DATA FROM KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "images = tf.keras.utils.image_dataset_from_directory(\n",
    "  \"/Users/billyluqiu/Desktop/fall2022/ece588Final/billy/images\", batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7038 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "images2 = tf.keras.utils.image_dataset_from_directory(\n",
    "  \"/Users/billyluqiu/Desktop/fall2022/ece588Final/billy/images2\", batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = images.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = images2.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (1, 256, 256, 3), types: tf.float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billyluqiu/miniforge3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "gen_g_optimizer = gen_f_optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "dis_x_optimizer = dis_y_optimizer = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Losses\n",
    "loss = BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Measures how close to one real images are rated, and how close to zero fake images are rated\n",
    "def discriminator_loss(real, generated):\n",
    "    # Multiplied by 0.5 so that it will train at half-speed\n",
    "    return (loss(tf.ones_like(real), real) + loss(tf.zeros_like(generated), generated)) * 0.5\n",
    "\n",
    "# Measures how real the discriminator believes the fake image is\n",
    "def gen_loss(validity):\n",
    "    return loss(tf.ones_like(validity), validity)\n",
    "\n",
    "# Measures similarity of two images.  Used for cycle and identity loss\n",
    "def image_similarity(image1, image2):\n",
    "    return tf.reduce_mean(tf.abs(image1 - image2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "    return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_generated = []\n",
    "mse_original = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_f_import.compile(gen_f_optimizer, generator_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 12:47:58.759979: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/Users/billyluqiu/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1366, in test_function  *\n        return step_function(self, iterator)\n\n    TypeError: tf__generator_loss() takes 1 positional argument but 2 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m y_hat \u001b[39m=\u001b[39m generator_f_import\u001b[39m.\u001b[39mpredict(x_numpy)\n\u001b[1;32m      5\u001b[0m x_hat \u001b[39m=\u001b[39m generator_g_import\u001b[39m.\u001b[39mpredict(y_hat)\n\u001b[0;32m----> 6\u001b[0m loss, acc \u001b[39m=\u001b[39m generator_f_import\u001b[39m.\u001b[39;49mevaluate(x_numpy, y_hat)\n\u001b[1;32m      7\u001b[0m loss_arr\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m      8\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/Users/billyluqiu/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\", line 1366, in test_function  *\n        return step_function(self, iterator)\n\n    TypeError: tf__generator_loss() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for x in test_x:\n",
    "    x_numpy = x.numpy().reshape((1, img_rows, img_cols, channels))\n",
    "    y_hat = generator_f_import.predict(x_numpy)\n",
    "    x_hat = generator_g_import.predict(y_hat)\n",
    "    loss, acc = generator_f_import.evaluate(x_numpy, y_hat)\n",
    "    loss_arr.append(loss)\n",
    "    break\n",
    "    # normalizedData = (y_hat[0]-np.min(y_hat[0]))/(np.max(y_hat[0])-np.min(y_hat[0]))\n",
    "\n",
    "    #matplotlib.image.imsave('test_images/'+ str(i) + '.png', normalizedData)\n",
    "\n",
    "    mse_generated.append(np.mean((y_hat[0] - x_hat[0])**2))\n",
    "    mse_original.append(np.mean((x_numpy[0] - x_hat[0])**2))\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1918379,\n",
       " 0.08098396,\n",
       " 0.008003573,\n",
       " 0.19106005,\n",
       " 0.041483726,\n",
       " 0.00408264,\n",
       " 0.010244875,\n",
       " 0.00543094,\n",
       " 0.054560483,\n",
       " 0.0031840252,\n",
       " 0.002741433,\n",
       " 0.0051574497,\n",
       " 0.0067718825,\n",
       " 0.03314799,\n",
       " 0.1970693,\n",
       " 0.012932452,\n",
       " 0.0023007528,\n",
       " 0.12739904,\n",
       " 0.11794257,\n",
       " 0.16828424,\n",
       " 0.011602341,\n",
       " 0.0027471727,\n",
       " 0.12687859,\n",
       " 0.046193898,\n",
       " 0.0039617363,\n",
       " 0.13887095,\n",
       " 0.21197003,\n",
       " 0.1980151,\n",
       " 0.003734435,\n",
       " 0.14060457,\n",
       " 0.015147518,\n",
       " 0.004062586,\n",
       " 0.026681326,\n",
       " 0.18611683,\n",
       " 0.21460311,\n",
       " 0.003949294,\n",
       " 0.20130378,\n",
       " 0.19986929,\n",
       " 0.20586963,\n",
       " 0.19279419,\n",
       " 0.20566566,\n",
       " 0.0031631363,\n",
       " 0.021363804,\n",
       " 0.17827313,\n",
       " 0.009771048,\n",
       " 0.0022726974,\n",
       " 0.010081521,\n",
       " 0.004510397,\n",
       " 0.17493738,\n",
       " 0.009489584,\n",
       " 0.15632261,\n",
       " 0.15676476,\n",
       " 0.17300843,\n",
       " 0.0020959063,\n",
       " 0.16653179,\n",
       " 0.065373324,\n",
       " 0.116181485,\n",
       " 0.010255174,\n",
       " 0.028394423,\n",
       " 0.0022810863,\n",
       " 0.0055262386,\n",
       " 0.003092631,\n",
       " 0.0037504097,\n",
       " 0.19844878,\n",
       " 0.014200155,\n",
       " 0.18019708,\n",
       " 0.0054677376,\n",
       " 0.1270824,\n",
       " 0.21276693,\n",
       " 0.172561,\n",
       " 0.046776634,\n",
       " 0.0037158926,\n",
       " 0.0039986013,\n",
       " 0.23015052,\n",
       " 0.040119298,\n",
       " 0.15219848,\n",
       " 0.22313082,\n",
       " 0.0513909,\n",
       " 0.12607263,\n",
       " 0.10620284,\n",
       " 0.015045953,\n",
       " 0.004779628,\n",
       " 0.20788856,\n",
       " 0.0058562923,\n",
       " 0.16295272,\n",
       " 0.19968349,\n",
       " 0.14545788,\n",
       " 0.019446462,\n",
       " 0.04588467,\n",
       " 0.0087224925,\n",
       " 0.0033544858,\n",
       " 0.0027868215,\n",
       " 0.008331577,\n",
       " 0.18384142,\n",
       " 0.0024301177,\n",
       " 0.015523009,\n",
       " 0.0061413986,\n",
       " 0.059006516,\n",
       " 0.053321302,\n",
       " 0.18470077,\n",
       " 0.06703217,\n",
       " 0.018392911,\n",
       " 0.20015411,\n",
       " 0.008018105,\n",
       " 0.011340684,\n",
       " 0.0043493765,\n",
       " 0.16467808,\n",
       " 0.1483753,\n",
       " 0.20145757,\n",
       " 0.002873582,\n",
       " 0.049717832,\n",
       " 0.0023931926,\n",
       " 0.055519234,\n",
       " 0.20970617,\n",
       " 0.0024454545,\n",
       " 0.13499461,\n",
       " 0.14923708,\n",
       " 0.03748983,\n",
       " 0.08640421,\n",
       " 0.0059209224,\n",
       " 0.0029433954,\n",
       " 0.0069930027,\n",
       " 0.14613451,\n",
       " 0.22846799,\n",
       " 0.041332465,\n",
       " 0.050045837,\n",
       " 0.003315644,\n",
       " 0.0018803459,\n",
       " 0.1907736,\n",
       " 0.23961182,\n",
       " 0.20056736,\n",
       " 0.14164682,\n",
       " 0.19139136,\n",
       " 0.20400053,\n",
       " 0.16640013,\n",
       " 0.13343559,\n",
       " 0.0023641398,\n",
       " 0.20910348,\n",
       " 0.0052379617,\n",
       " 0.001985064,\n",
       " 0.04266234,\n",
       " 0.15209222,\n",
       " 0.19059408,\n",
       " 0.078603975,\n",
       " 0.2206197,\n",
       " 0.18727522,\n",
       " 0.23536772,\n",
       " 0.22306752,\n",
       " 0.20626067,\n",
       " 0.20514967,\n",
       " 0.05659334,\n",
       " 0.0039363443,\n",
       " 0.015250039,\n",
       " 0.13906972,\n",
       " 0.19505258,\n",
       " 0.046455752,\n",
       " 0.07074567,\n",
       " 0.004480799,\n",
       " 0.18058263,\n",
       " 0.179805,\n",
       " 0.2506213,\n",
       " 0.1620469,\n",
       " 0.13331945,\n",
       " 0.08270051,\n",
       " 0.005196146,\n",
       " 0.037968148,\n",
       " 0.019816043,\n",
       " 0.004238578,\n",
       " 0.17871897,\n",
       " 0.07806445,\n",
       " 0.0035924064,\n",
       " 0.18491273,\n",
       " 0.18894863,\n",
       " 0.013830767,\n",
       " 0.19381692,\n",
       " 0.013343952,\n",
       " 0.035922788,\n",
       " 0.004375433,\n",
       " 0.0061169737,\n",
       " 0.21490292,\n",
       " 0.003370314,\n",
       " 0.034046803,\n",
       " 0.18012792,\n",
       " 0.042222586,\n",
       " 0.011445564,\n",
       " 0.112891905,\n",
       " 0.054311395,\n",
       " 0.004680631,\n",
       " 0.24819095,\n",
       " 0.18399744,\n",
       " 0.1918117,\n",
       " 0.21549802,\n",
       " 0.034417544,\n",
       " 0.010483139,\n",
       " 0.16681594,\n",
       " 0.009315658,\n",
       " 0.015367753,\n",
       " 0.22759725,\n",
       " 0.044746604,\n",
       " 0.20556904,\n",
       " 0.079275884,\n",
       " 0.17772484,\n",
       " 0.15965895,\n",
       " 0.00858108,\n",
       " 0.22200084,\n",
       " 0.18052839,\n",
       " 0.19440061,\n",
       " 0.027953656,\n",
       " 0.02901022,\n",
       " 0.20142446,\n",
       " 0.18853666,\n",
       " 0.0021999842,\n",
       " 0.0032208606,\n",
       " 0.116729476,\n",
       " 0.12455866,\n",
       " 0.17056365,\n",
       " 0.15795456,\n",
       " 0.025945557,\n",
       " 0.018881554,\n",
       " 0.099807315,\n",
       " 0.0037537615,\n",
       " 0.19161093,\n",
       " 0.005756313,\n",
       " 0.029397927,\n",
       " 0.2265049,\n",
       " 0.007004941,\n",
       " 0.04687902,\n",
       " 0.05701479,\n",
       " 0.003487257,\n",
       " 0.0054910798,\n",
       " 0.015258439,\n",
       " 0.111504674,\n",
       " 0.0021276595,\n",
       " 0.022024617,\n",
       " 0.03793489,\n",
       " 0.006849734,\n",
       " 0.14266676,\n",
       " 0.0039016323,\n",
       " 0.20898622,\n",
       " 0.039993104,\n",
       " 0.0068758763,\n",
       " 0.0018699201,\n",
       " 0.004921282,\n",
       " 0.15744324,\n",
       " 0.0032328248,\n",
       " 0.13542435,\n",
       " 0.0043109497,\n",
       " 0.004539642,\n",
       " 0.22817476,\n",
       " 0.0561385,\n",
       " 0.08091267,\n",
       " 0.17939188,\n",
       " 0.18028785,\n",
       " 0.008416209,\n",
       " 0.10157579,\n",
       " 0.023265226,\n",
       " 0.20314908,\n",
       " 0.07669771,\n",
       " 0.17866087,\n",
       " 0.014206833,\n",
       " 0.12624341,\n",
       " 0.20257927,\n",
       " 0.010835794,\n",
       " 0.1912537,\n",
       " 0.026671095,\n",
       " 0.19801797,\n",
       " 0.0085849315,\n",
       " 0.0025753493,\n",
       " 0.23702693,\n",
       " 0.24132645,\n",
       " 0.009946818,\n",
       " 0.0234823,\n",
       " 0.16958122,\n",
       " 0.0073196236,\n",
       " 0.18520552,\n",
       " 0.03806771,\n",
       " 0.023222657,\n",
       " 0.009735874,\n",
       " 0.012793429,\n",
       " 0.07989052,\n",
       " 0.18058895,\n",
       " 0.0043280176,\n",
       " 0.005609675,\n",
       " 0.005466664,\n",
       " 0.010494058,\n",
       " 0.0029420406,\n",
       " 0.009958561,\n",
       " 0.15107892,\n",
       " 0.1943308,\n",
       " 0.008493508,\n",
       " 0.016263517,\n",
       " 0.0043353117,\n",
       " 0.088340916,\n",
       " 0.16219686,\n",
       " 0.1852334,\n",
       " 0.0064709852,\n",
       " 0.09852865,\n",
       " 0.22288261,\n",
       " 0.009190521,\n",
       " 0.1417536,\n",
       " 0.010736265,\n",
       " 0.0202517,\n",
       " 0.0039644875,\n",
       " 0.1721953,\n",
       " 0.22380443,\n",
       " 0.20215072,\n",
       " 0.0026027746,\n",
       " 0.20646709,\n",
       " 0.08085875,\n",
       " 0.0059258267,\n",
       " 0.020053597,\n",
       " 0.0030822011,\n",
       " 0.17503257,\n",
       " 0.20973746,\n",
       " 0.16231358,\n",
       " 0.028327977,\n",
       " 0.0020541002,\n",
       " 0.21291839,\n",
       " 0.023333272,\n",
       " 0.005338393,\n",
       " 0.005454958,\n",
       " 0.053631958,\n",
       " 0.21873367,\n",
       " 0.1868907,\n",
       " 0.13345426,\n",
       " 0.14862321,\n",
       " 0.18104267,\n",
       " 0.013318882,\n",
       " 0.01237294,\n",
       " 0.013565683,\n",
       " 0.08453093,\n",
       " 0.18943714,\n",
       " 0.1448409,\n",
       " 0.16090438,\n",
       " 0.06116101,\n",
       " 0.0028865163,\n",
       " 0.23048885,\n",
       " 0.0118215745,\n",
       " 0.028275354,\n",
       " 0.012866825,\n",
       " 0.09018851,\n",
       " 0.17789046,\n",
       " 0.017444223,\n",
       " 0.22275835,\n",
       " 0.0020798922,\n",
       " 0.020249806,\n",
       " 0.020211777,\n",
       " 0.008434621,\n",
       " 0.07619849,\n",
       " 0.0035089247,\n",
       " 0.19263291,\n",
       " 0.18799822,\n",
       " 0.12164653,\n",
       " 0.032291126,\n",
       " 0.19749634,\n",
       " 0.22335821,\n",
       " 0.16867243,\n",
       " 0.16380095,\n",
       " 0.053919893,\n",
       " 0.003311423,\n",
       " 0.00562471,\n",
       " 0.22305997,\n",
       " 0.0021697173,\n",
       " 0.21996401,\n",
       " 0.028832091,\n",
       " 0.18871649,\n",
       " 0.0036865333,\n",
       " 0.009298396,\n",
       " 0.0678347,\n",
       " 0.05918388,\n",
       " 0.16720803,\n",
       " 0.1679226,\n",
       " 0.097811304,\n",
       " 0.19633655,\n",
       " 0.004646268,\n",
       " 0.17446096,\n",
       " 0.026518531,\n",
       " 0.15593158,\n",
       " 0.054526076,\n",
       " 0.014586236,\n",
       " 0.0048591937,\n",
       " 0.036910977,\n",
       " 0.002905868,\n",
       " 0.0027137334,\n",
       " 0.18046075,\n",
       " 0.022553386,\n",
       " 0.00825575,\n",
       " 0.1445627,\n",
       " 0.03662539,\n",
       " 0.003348434,\n",
       " 0.033280935,\n",
       " 0.07239682,\n",
       " 0.09654567,\n",
       " 0.06636895,\n",
       " 0.0032770273,\n",
       " 0.011741652,\n",
       " 0.004322559,\n",
       " 0.0066276924,\n",
       " 0.13937695,\n",
       " 0.207999,\n",
       " 0.019743167,\n",
       " 0.18168022,\n",
       " 0.091151804,\n",
       " 0.01777477,\n",
       " 0.01755855,\n",
       " 0.18180038,\n",
       " 0.013538711,\n",
       " 0.18236744,\n",
       " 0.0093620615,\n",
       " 0.13784383,\n",
       " 0.23373719,\n",
       " 0.21824849,\n",
       " 0.17797518,\n",
       " 0.035397526,\n",
       " 0.009999555,\n",
       " 0.08654841,\n",
       " 0.005199981,\n",
       " 0.07282805,\n",
       " 0.029594503,\n",
       " 0.013866416,\n",
       " 0.22557002,\n",
       " 0.072816886,\n",
       " 0.04574878,\n",
       " 0.15657629,\n",
       " 0.006467232,\n",
       " 0.011157054,\n",
       " 0.23584746,\n",
       " 0.0043594968,\n",
       " 0.19959651,\n",
       " 0.20653796,\n",
       " 0.0067281467,\n",
       " 0.121893585,\n",
       " 0.024748482,\n",
       " 0.004081599,\n",
       " 0.17076391,\n",
       " 0.005924527,\n",
       " 0.18745588,\n",
       " 0.26058617,\n",
       " 0.0024877817,\n",
       " 0.043924954,\n",
       " 0.21209772,\n",
       " 0.0022970617,\n",
       " 0.017036999,\n",
       " 0.18755375,\n",
       " 0.19367498,\n",
       " 0.16221352,\n",
       " 0.21420689,\n",
       " 0.003941721,\n",
       " 0.143281,\n",
       " 0.1880814,\n",
       " 0.01626719,\n",
       " 0.17665996,\n",
       " 0.16576914,\n",
       " 0.008963066,\n",
       " 0.0057503725,\n",
       " 0.20045425,\n",
       " 0.18224607,\n",
       " 0.07689946,\n",
       " 0.19826277,\n",
       " 0.020052657,\n",
       " 0.16118248,\n",
       " 0.20356463,\n",
       " 0.164528,\n",
       " 0.18818879,\n",
       " 0.070705526,\n",
       " 0.12811215,\n",
       " 0.1718509,\n",
       " 0.14656855,\n",
       " 0.1362874,\n",
       " 0.12601964,\n",
       " 0.011585884,\n",
       " 0.054858055,\n",
       " 0.089547396,\n",
       " 0.10736218,\n",
       " 0.0030467964,\n",
       " 0.0024626437,\n",
       " 0.0055842283,\n",
       " 0.2269057,\n",
       " 0.08052323,\n",
       " 0.18936844,\n",
       " 0.10937246,\n",
       " 0.04636887,\n",
       " 0.012056123,\n",
       " 0.19865789,\n",
       " 0.13702573,\n",
       " 0.0067136227,\n",
       " 0.023681479,\n",
       " 0.04960884,\n",
       " 0.016983323,\n",
       " 0.018862313,\n",
       " 0.010860841,\n",
       " 0.011027793,\n",
       " 0.15391237,\n",
       " 0.15359339,\n",
       " 0.0020584574,\n",
       " 0.18581481,\n",
       " 0.065546215,\n",
       " 0.15064937,\n",
       " 0.0032639417,\n",
       " 0.18746881,\n",
       " 0.17159836,\n",
       " 0.17325193,\n",
       " 0.10247516,\n",
       " 0.25785,\n",
       " 0.14335848,\n",
       " 0.06307118,\n",
       " 0.19537745,\n",
       " 0.011856802,\n",
       " 0.0065741804,\n",
       " 0.22364534,\n",
       " 0.007629127,\n",
       " 0.20020515,\n",
       " 0.0057700165,\n",
       " 0.0039798752,\n",
       " 0.18906921,\n",
       " 0.003902088,\n",
       " 0.2204656,\n",
       " 0.0088021625,\n",
       " 0.0074628443,\n",
       " 0.063859455,\n",
       " 0.13444488,\n",
       " 0.011457742,\n",
       " 0.0033011565,\n",
       " 0.21201152,\n",
       " 0.0055259713,\n",
       " 0.005661261,\n",
       " 0.1927915,\n",
       " 0.002595761,\n",
       " 0.005066932,\n",
       " 0.031335592,\n",
       " 0.020564472,\n",
       " 0.019775493,\n",
       " 0.0029455374,\n",
       " 0.003626353,\n",
       " 0.21475947,\n",
       " 0.0047863936,\n",
       " 0.021066705,\n",
       " 0.0672125,\n",
       " 0.21283732,\n",
       " 0.16493475,\n",
       " 0.22506535,\n",
       " 0.061051782,\n",
       " 0.010092167,\n",
       " 0.1224198,\n",
       " 0.21826057,\n",
       " 0.017656256,\n",
       " 0.13894969,\n",
       " 0.05240345,\n",
       " 0.0064779776,\n",
       " 0.0034189683,\n",
       " 0.101871856,\n",
       " 0.010125756,\n",
       " 0.0037757352,\n",
       " 0.19128424,\n",
       " 0.004965033,\n",
       " 0.002092758,\n",
       " 0.0039156713,\n",
       " 0.14665742,\n",
       " 0.12423266,\n",
       " 0.00876107,\n",
       " 0.011355548,\n",
       " 0.0062240916,\n",
       " 0.0075420267,\n",
       " 0.0057472424,\n",
       " 0.003194854,\n",
       " 0.014981537,\n",
       " 0.19555324,\n",
       " 0.16919003,\n",
       " 0.0038142216,\n",
       " 0.19589943,\n",
       " 0.07974435,\n",
       " 0.06742864,\n",
       " 0.026319126,\n",
       " 0.008174317,\n",
       " 0.029274443,\n",
       " 0.17309405,\n",
       " 0.008569415,\n",
       " 0.16014175,\n",
       " 0.0046831705,\n",
       " 0.020640047,\n",
       " 0.014802665,\n",
       " 0.0045353333,\n",
       " 0.16221295,\n",
       " 0.0046294853,\n",
       " 0.16410373,\n",
       " 0.017448038,\n",
       " 0.19878227,\n",
       " 0.118862234,\n",
       " 0.006954094,\n",
       " 0.18989472,\n",
       " 0.014669623,\n",
       " 0.003543514,\n",
       " 0.12207387,\n",
       " 0.11315161,\n",
       " 0.014894374,\n",
       " 0.026936732,\n",
       " 0.22822177,\n",
       " 0.1429698,\n",
       " 0.017529605,\n",
       " 0.04980266,\n",
       " 0.018443,\n",
       " 0.05365995,\n",
       " 0.0032993488,\n",
       " 0.035521675,\n",
       " 0.0029618833,\n",
       " 0.2344451,\n",
       " 0.12907054,\n",
       " 0.0038463585,\n",
       " 0.20296864,\n",
       " 0.0054552886,\n",
       " 0.2377006,\n",
       " 0.019224776,\n",
       " 0.04373392,\n",
       " 0.2005362,\n",
       " 0.0022445994,\n",
       " 0.13633555,\n",
       " 0.18708284,\n",
       " 0.0025193614,\n",
       " 0.15969263,\n",
       " 0.028006839,\n",
       " 0.18738832,\n",
       " 0.17913897,\n",
       " 0.16386545,\n",
       " 0.007144848,\n",
       " 0.10236246,\n",
       " 0.23453875,\n",
       " 0.03789488,\n",
       " 0.09452193,\n",
       " 0.023568323,\n",
       " 0.0055305883,\n",
       " 0.21958177,\n",
       " 0.037596546,\n",
       " 0.005527252,\n",
       " 0.13475554,\n",
       " 0.014033034,\n",
       " 0.015313767,\n",
       " 0.2007974,\n",
       " 0.0032636758,\n",
       " 0.059049487,\n",
       " 0.2039482,\n",
       " 0.004117283,\n",
       " 0.1949131,\n",
       " 0.19472826,\n",
       " 0.042532522,\n",
       " 0.0031886986,\n",
       " 0.14395082,\n",
       " 0.16404589,\n",
       " 0.01109973,\n",
       " 0.012368708,\n",
       " 0.154192,\n",
       " 0.17538051,\n",
       " 0.022485564,\n",
       " 0.095659256,\n",
       " 0.17158157,\n",
       " 0.0068233446,\n",
       " 0.121442854,\n",
       " 0.02435335,\n",
       " 0.0036366328,\n",
       " 0.028741404,\n",
       " 0.0051866802,\n",
       " 0.010433366,\n",
       " 0.0040489915,\n",
       " 0.0065147565,\n",
       " 0.0076864585,\n",
       " 0.168263,\n",
       " 0.058956712,\n",
       " 0.003886908,\n",
       " 0.012009873,\n",
       " 0.029260546,\n",
       " 0.1802258,\n",
       " 0.004783762,\n",
       " 0.012070737,\n",
       " 0.16739048,\n",
       " 0.026679663,\n",
       " 0.19132674,\n",
       " 0.12005951,\n",
       " 0.004218479,\n",
       " 0.033535462,\n",
       " 0.00430314,\n",
       " 0.00434989,\n",
       " 0.0036334537,\n",
       " 0.023605736,\n",
       " 0.02397383,\n",
       " 0.0024700633,\n",
       " 0.0049499916,\n",
       " 0.033975657,\n",
       " 0.0024991224,\n",
       " 0.16131674,\n",
       " 0.0058018886,\n",
       " 0.12176863,\n",
       " 0.09264297,\n",
       " 0.037415735,\n",
       " 0.0027704341,\n",
       " 0.01566275,\n",
       " 0.00285149,\n",
       " 0.18843496,\n",
       " 0.15900373,\n",
       " 0.013264288,\n",
       " 0.022517232,\n",
       " 0.006361449,\n",
       " 0.09551397,\n",
       " 0.06624504,\n",
       " 0.15955903,\n",
       " 0.17088573,\n",
       " 0.012837955,\n",
       " 0.03292555,\n",
       " 0.16024165,\n",
       " 0.022791782,\n",
       " 0.028201222,\n",
       " 0.2290193,\n",
       " 0.19125794,\n",
       " 0.17513537,\n",
       " 0.010924895,\n",
       " 0.16362773,\n",
       " 0.0040813964,\n",
       " 0.15896042,\n",
       " 0.0029714021,\n",
       " 0.14313783,\n",
       " 0.17336375,\n",
       " 0.0030786647,\n",
       " 0.0033073572,\n",
       " 0.21518397,\n",
       " 0.15268761,\n",
       " 0.114724554,\n",
       " 0.23527122,\n",
       " 0.21424675,\n",
       " 0.0028159574,\n",
       " 0.19300924,\n",
       " 0.17813194,\n",
       " 0.098909885,\n",
       " 0.1944745,\n",
       " 0.009331518,\n",
       " 0.12659638,\n",
       " 0.016546234,\n",
       " 0.16422288,\n",
       " 0.0037039516,\n",
       " 0.114737056,\n",
       " 0.20602451,\n",
       " 0.006880338,\n",
       " 0.009957208,\n",
       " 0.15168598,\n",
       " 0.053974416,\n",
       " 0.19688444,\n",
       " 0.023212634,\n",
       " 0.11979473,\n",
       " 0.041253895,\n",
       " 0.004984893,\n",
       " 0.15498203,\n",
       " 0.038660616,\n",
       " 0.21222913,\n",
       " 0.15073653,\n",
       " 0.00530252,\n",
       " 0.22347015,\n",
       " 0.18595211,\n",
       " 0.0053687505,\n",
       " 0.04259253,\n",
       " 0.011711377,\n",
       " 0.1941976,\n",
       " 0.01723208,\n",
       " 0.188568,\n",
       " 0.20685454,\n",
       " 0.0038920129,\n",
       " 0.009740338,\n",
       " 0.0023525392,\n",
       " 0.13799717,\n",
       " 0.037041325,\n",
       " 0.008010388,\n",
       " 0.1957608,\n",
       " 0.0020603293,\n",
       " 0.009237133,\n",
       " 0.10372281,\n",
       " 0.0051325555,\n",
       " 0.0107999565,\n",
       " 0.0049674963,\n",
       " 0.07868875,\n",
       " 0.14213888,\n",
       " 0.03429761,\n",
       " 0.005846337,\n",
       " 0.014253303,\n",
       " 0.018601684,\n",
       " 0.025503004,\n",
       " 0.019322746,\n",
       " 0.009982002,\n",
       " 0.004423736,\n",
       " 0.15658668,\n",
       " 0.0081245825,\n",
       " 0.04300623,\n",
       " 0.007786606,\n",
       " 0.014482903,\n",
       " 0.1014457,\n",
       " 0.025167808,\n",
       " 0.18429823,\n",
       " 0.17024015,\n",
       " 0.0108504435,\n",
       " 0.008092258,\n",
       " 0.22145553,\n",
       " 0.007807251,\n",
       " 0.012194472,\n",
       " 0.21481203,\n",
       " 0.03794841,\n",
       " 0.1680878,\n",
       " 0.0023935628,\n",
       " 0.09067044,\n",
       " 0.13699357,\n",
       " 0.14617,\n",
       " 0.26982966,\n",
       " 0.03921587,\n",
       " 0.008652282,\n",
       " 0.19726579,\n",
       " 0.19800766,\n",
       " 0.016609287,\n",
       " 0.19375217,\n",
       " 0.051651344,\n",
       " 0.32251185,\n",
       " 0.21654384,\n",
       " 0.019352593,\n",
       " 0.09526508,\n",
       " 0.0073067676,\n",
       " 0.04198353,\n",
       " 0.063383415,\n",
       " 0.007905313,\n",
       " 0.12607689,\n",
       " 0.17752963,\n",
       " 0.00392528,\n",
       " 0.09947454,\n",
       " 0.15207537,\n",
       " 0.0052215196,\n",
       " 0.0065461635,\n",
       " 0.040003683,\n",
       " 0.022656402,\n",
       " 0.017936857,\n",
       " 0.19995366,\n",
       " 0.0041310685,\n",
       " 0.2009588,\n",
       " 0.12512015,\n",
       " 0.16091512,\n",
       " 0.12044064,\n",
       " 0.15110222,\n",
       " 0.006322012,\n",
       " 0.0147347255,\n",
       " 0.01824209,\n",
       " 0.10216876,\n",
       " 0.0025265524,\n",
       " 0.20116763,\n",
       " 0.06438493,\n",
       " 0.19049709,\n",
       " 0.0050114277,\n",
       " 0.1360298,\n",
       " 0.1593246,\n",
       " 0.18051325,\n",
       " 0.017835153,\n",
       " 0.19536982,\n",
       " 0.17331956,\n",
       " 0.004051987,\n",
       " 0.044856578,\n",
       " 0.08835266,\n",
       " 0.0039451257,\n",
       " 0.010177764,\n",
       " 0.031098368,\n",
       " 0.019803869,\n",
       " 0.00324845,\n",
       " 0.19336264,\n",
       " 0.0052557476,\n",
       " 0.1603602,\n",
       " 0.15647393,\n",
       " 0.09694472,\n",
       " 0.0030073673,\n",
       " 0.0125635825,\n",
       " 0.15111208,\n",
       " 0.0027040306,\n",
       " 0.050755907,\n",
       " 0.14128183,\n",
       " 0.11288849,\n",
       " 0.09189672,\n",
       " 0.08304963,\n",
       " 0.08529493,\n",
       " 0.0028235295,\n",
       " 0.0029390715,\n",
       " 0.017563095,\n",
       " 0.005803739,\n",
       " 0.0045772665,\n",
       " 0.009042012,\n",
       " 0.001847295,\n",
       " 0.1816697,\n",
       " 0.004155293,\n",
       " 0.0022678825,\n",
       " 0.14407144,\n",
       " 0.0023259025,\n",
       " 0.18935926,\n",
       " 0.100013316,\n",
       " 0.0051708343,\n",
       " 0.01053588,\n",
       " 0.12537849,\n",
       " 0.15896599,\n",
       " 0.10825759,\n",
       " 0.15576853,\n",
       " 0.019035397,\n",
       " 0.0024064803,\n",
       " 0.14377828,\n",
       " 0.0030966362,\n",
       " 0.20299615,\n",
       " 0.24204172,\n",
       " 0.10861366,\n",
       " 0.12816213,\n",
       " 0.20927668,\n",
       " 0.14566168,\n",
       " 0.16833758,\n",
       " 0.005817097,\n",
       " 0.04008161,\n",
       " 0.18600565,\n",
       " 0.17808448,\n",
       " 0.025738783,\n",
       " 0.010727513,\n",
       " 0.0069715423,\n",
       " 0.039219,\n",
       " 0.09564949,\n",
       " 0.016872417,\n",
       " 0.031248182,\n",
       " 0.006231466,\n",
       " 0.18370013,\n",
       " 0.08703888,\n",
       " 0.15268375,\n",
       " 0.1664057,\n",
       " 0.053501427,\n",
       " 0.03725334,\n",
       " 0.026104325,\n",
       " 0.0037624377,\n",
       " 0.1813702,\n",
       " 0.02621659,\n",
       " 0.004532047,\n",
       " 0.201931,\n",
       " 0.00326246,\n",
       " 0.0045526573,\n",
       " 0.20197003,\n",
       " 0.0060433075,\n",
       " 0.21003656,\n",
       " 0.2054881,\n",
       " 0.16065162,\n",
       " 0.009706564,\n",
       " 0.103870966,\n",
       " 0.06678801,\n",
       " 0.18245666,\n",
       " 0.013394486,\n",
       " 0.002867868,\n",
       " 0.1877749,\n",
       " 0.17300622,\n",
       " 0.0029217966,\n",
       " 0.06718189,\n",
       " 0.19037421,\n",
       " 0.0020187513,\n",
       " 0.2329006,\n",
       " 0.018602984,\n",
       " 0.14458065,\n",
       " 0.0030749638,\n",
       " 0.1486637,\n",
       " 0.0076994463,\n",
       " 0.132654,\n",
       " 0.19712771,\n",
       " 0.1559339,\n",
       " 0.0031359692,\n",
       " 0.11026604,\n",
       " 0.017968968,\n",
       " 0.084697075,\n",
       " 0.18144202,\n",
       " 0.01573841,\n",
       " 0.18792695,\n",
       " 0.0026122036,\n",
       " 0.16116782,\n",
       " 0.0108719915,\n",
       " 0.18808307,\n",
       " 0.01584276,\n",
       " 0.002276203,\n",
       " 0.18178463,\n",
       " 0.19155209,\n",
       " 0.22350742,\n",
       " 0.020383963,\n",
       " 0.16157614,\n",
       " 0.0200487,\n",
       " 0.0035587887,\n",
       " 0.0126785785,\n",
       " 0.13457634,\n",
       " 0.1829201,\n",
       " 0.21383502,\n",
       " 0.004635872,\n",
       " 0.0925362,\n",
       " 0.02075599,\n",
       " 0.005890505,\n",
       " 0.023884282,\n",
       " 0.060928494,\n",
       " 0.010319197,\n",
       " 0.034391884,\n",
       " 0.057430852,\n",
       " 0.17229249,\n",
       " 0.114576064,\n",
       " 0.013296555,\n",
       " 0.0040053898,\n",
       " 0.050071448,\n",
       " 0.0047985096,\n",
       " 0.003502707,\n",
       " 0.029945403,\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02586645"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((x_numpy[0] - x_hat[0])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.753e+03, 1.225e+03, 5.200e+01, 2.000e+00, 2.000e+00, 2.000e+00,\n",
       "        1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
       " array([0.00158216, 0.02318895, 0.04479573, 0.06640252, 0.08800931,\n",
       "        0.10961609, 0.13122287, 0.15282966, 0.17443644, 0.19604322,\n",
       "        0.21765001], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "axs[0].hist(mse_generated)\n",
    "axs[1].hist(mse_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38fe08f3d26a479ec4b53fd1eed47e85ad3eb35a4419302a8faf528c63e8efb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
