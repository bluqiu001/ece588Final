{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billyluqiu/miniforge3/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import (Activation, Concatenate, Conv2D,\n",
    "                                     Conv2DTranspose, Input, LeakyReLU)\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 15:19:46.317375: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-06 15:19:46.317496: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "generator_g_import = tf.keras.models.load_model('generated_models/generator_g.h5')\n",
    "generator_f_import = tf.keras.models.load_model('generated_models/generator_f.h5')\n",
    "discriminator_x_import = tf.keras.models.load_model('generated_models/discriminator_x.h5')\n",
    "discriminator_y_import = tf.keras.models.load_model('generated_models/discriminator_y.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data, metadata = tfds.load('cycle_gan/monet2photo', with_info=True, as_supervised=True)\n",
    "\n",
    "train_x, train_y, test_x, test_y = data['trainA'], data['trainB'], data['testA'], data['testB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billyluqiu/miniforge3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "epochs = 50\n",
    "\n",
    "LAMBDA = 10\n",
    "\n",
    "img_rows, img_cols, channels = 256, 256, 3\n",
    "weight_initializer = RandomNormal(stddev=0.02)\n",
    "\n",
    "gen_g_optimizer = gen_f_optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "dis_x_optimizer = dis_y_optimizer = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images to [-1, 1] and reshape\n",
    "def preprocess_image(image, _):\n",
    "    return tf.reshape(tf.cast(tf.image.resize(image, (int(img_rows), int(img_cols))), tf.float32) / 127.5 - 1, (1, img_rows, img_cols, channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the normalization onto the dataset\n",
    "train_x = train_x.map(preprocess_image)\n",
    "train_y = train_y.map(preprocess_image)\n",
    "test_x = test_x.map(preprocess_image)\n",
    "test_y = test_y.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (1, 256, 256, 3), types: tf.float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images():\n",
    "    # Sample images\n",
    "    print(test_x)\n",
    "    x = next(iter(test_x.shuffle(1000))).numpy()\n",
    "    y = next(iter(test_y.shuffle(1000))).numpy()\n",
    "    print(x)\n",
    "    # Get predictions for those images\n",
    "    y_hat = generator_g_import.predict(x.reshape((1, img_rows, img_cols, channels)))\n",
    "    x_hat = generator_f_import.predict(y.reshape((1, img_rows, img_cols, channels)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BELOW THIS IS THE TEST DATA FROM KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "images = tf.keras.utils.image_dataset_from_directory(\n",
    "  \"/Users/billyluqiu/Desktop/fall2022/ece588Final/billy/images\", batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7038 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "images2 = tf.keras.utils.image_dataset_from_directory(\n",
    "  \"/Users/billyluqiu/Desktop/fall2022/ece588Final/billy/images2\", batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = images.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = images2.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (1, 256, 256, 3), types: tf.float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_g_optimizer = gen_f_optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "dis_x_optimizer = dis_y_optimizer = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Losses\n",
    "loss = BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Measures how close to one real images are rated, and how close to zero fake images are rated\n",
    "def discriminator_loss(real, generated):\n",
    "    # Multiplied by 0.5 so that it will train at half-speed\n",
    "    return (loss(tf.ones_like(real), real) + loss(tf.zeros_like(generated), generated)) * 0.5\n",
    "\n",
    "# Measures how real the discriminator believes the fake image is\n",
    "def gen_loss(validity):\n",
    "    return loss(tf.ones_like(validity), validity)\n",
    "\n",
    "# Measures similarity of two images.  Used for cycle and identity loss\n",
    "def image_similarity(image1, image2):\n",
    "    return tf.reduce_mean(tf.abs(image1 - image2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "    return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_generated = []\n",
    "mse_original = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_f_import.compile(gen_f_optimizer, generator_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 15:19:56.834535: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-12-06 15:19:57.128207: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-06 15:19:57.742790: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for x in test_x:\n",
    "    x_numpy = x.numpy().reshape((1, img_rows, img_cols, channels))\n",
    "    y_hat = generator_f_import.predict(x_numpy)\n",
    "    x_hat = generator_g_import.predict(y_hat)\n",
    "    #loss, acc = generator_f_import.evaluate(x_numpy, y_hat)\n",
    "    # loss_arr.append(loss)\n",
    "    # break\n",
    "    normalizedData = (y_hat[0]-np.min(y_hat[0]))/(np.max(y_hat[0])-np.min(y_hat[0]))\n",
    "    normalizedDataX = np.array(x[0])\n",
    "    normalizedDataX = (normalizedDataX-np.min(normalizedDataX))/(np.max(normalizedDataX)-np.min(normalizedDataX))\n",
    "    matplotlib.image.imsave('test_images/'+ str(i) + '.png', normalizedData)\n",
    "    matplotlib.image.imsave('test_images_original/'+ str(i) + '.png', normalizedDataX)\n",
    "\n",
    "    mse_generated.append(np.mean((y_hat[0] - x_hat[0])**2))\n",
    "    mse_original.append(np.mean((x_numpy[0] - x_hat[0])**2))\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(normalizedDataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01283256,\n",
       " 0.109321214,\n",
       " 0.003621077,\n",
       " 0.0027041547,\n",
       " 0.043688405,\n",
       " 0.16390741,\n",
       " 0.18753783,\n",
       " 0.15491502,\n",
       " 0.17654347,\n",
       " 0.0031008201,\n",
       " 0.15366274,\n",
       " 0.009982002,\n",
       " 0.1223327,\n",
       " 0.1429698,\n",
       " 0.24584597,\n",
       " 0.0020541002,\n",
       " 0.0017358594,\n",
       " 0.048586965,\n",
       " 0.17656906,\n",
       " 0.00428399,\n",
       " 0.0060271877,\n",
       " 0.0023475445,\n",
       " 0.18837173,\n",
       " 0.060788095,\n",
       " 0.17368644,\n",
       " 0.004276885,\n",
       " 0.19741201,\n",
       " 0.023442864,\n",
       " 0.20566566,\n",
       " 0.017763289,\n",
       " 0.033172477,\n",
       " 0.0022246025,\n",
       " 0.0064963414,\n",
       " 0.20724855,\n",
       " 0.0076658386,\n",
       " 0.17871606,\n",
       " 0.17902137,\n",
       " 0.23859274,\n",
       " 0.20428367,\n",
       " 0.19159947,\n",
       " 0.17847393,\n",
       " 0.005772581,\n",
       " 0.17361636,\n",
       " 0.0036597075,\n",
       " 0.027673436,\n",
       " 0.0031058856,\n",
       " 0.01219191,\n",
       " 0.14339148,\n",
       " 0.010626872,\n",
       " 0.007550888,\n",
       " 0.0061880876,\n",
       " 0.008282796,\n",
       " 0.022308268,\n",
       " 0.0038801946,\n",
       " 0.16300128,\n",
       " 0.0058791935,\n",
       " 0.18777752,\n",
       " 0.20819902,\n",
       " 0.0071732886,\n",
       " 0.056164306,\n",
       " 0.0034032315,\n",
       " 0.19497494,\n",
       " 0.010066391,\n",
       " 0.068853624,\n",
       " 0.09203023,\n",
       " 0.22895402,\n",
       " 0.0211791,\n",
       " 0.032407247,\n",
       " 0.16013233,\n",
       " 0.16382456,\n",
       " 0.059911292,\n",
       " 0.1965871,\n",
       " 0.050953258,\n",
       " 0.032643337,\n",
       " 0.13941763,\n",
       " 0.16272478,\n",
       " 0.028261185,\n",
       " 0.056411583,\n",
       " 0.20732363,\n",
       " 0.16522127,\n",
       " 0.14944388,\n",
       " 0.019743167,\n",
       " 0.18281938,\n",
       " 0.008970567,\n",
       " 0.12822366,\n",
       " 0.0045014503,\n",
       " 0.17041959,\n",
       " 0.102128536,\n",
       " 0.0039057806,\n",
       " 0.029111242,\n",
       " 0.0019889444,\n",
       " 0.080787785,\n",
       " 0.019477278,\n",
       " 0.18792695,\n",
       " 0.011069417,\n",
       " 0.1426356,\n",
       " 0.020564472,\n",
       " 0.19217853,\n",
       " 0.14650314,\n",
       " 0.002353255,\n",
       " 0.13361186,\n",
       " 0.20416184,\n",
       " 0.22347015,\n",
       " 0.20890518,\n",
       " 0.17340364,\n",
       " 0.007118543,\n",
       " 0.036780875,\n",
       " 0.015476067,\n",
       " 0.1357439,\n",
       " 0.012480214,\n",
       " 0.0043109497,\n",
       " 0.08028117,\n",
       " 0.06049343,\n",
       " 0.010417274,\n",
       " 0.0062954645,\n",
       " 0.06636895,\n",
       " 0.1942833,\n",
       " 0.0056792,\n",
       " 0.004768625,\n",
       " 0.004411578,\n",
       " 0.08377362,\n",
       " 0.0142944055,\n",
       " 0.021218421,\n",
       " 0.2613812,\n",
       " 0.18579507,\n",
       " 0.009042012,\n",
       " 0.0665364,\n",
       " 0.15172444,\n",
       " 0.02156441,\n",
       " 0.17881012,\n",
       " 0.12811215,\n",
       " 0.22350742,\n",
       " 0.008238621,\n",
       " 0.006158251,\n",
       " 0.002012349,\n",
       " 0.2096964,\n",
       " 0.1600313,\n",
       " 0.15452942,\n",
       " 0.018602984,\n",
       " 0.020360768,\n",
       " 0.0073050465,\n",
       " 0.07074567,\n",
       " 0.168912,\n",
       " 0.21803083,\n",
       " 0.13833867,\n",
       " 0.05744006,\n",
       " 0.2098125,\n",
       " 0.0028940241,\n",
       " 0.17943913,\n",
       " 0.0028878546,\n",
       " 0.0072332523,\n",
       " 0.060263336,\n",
       " 0.0073466655,\n",
       " 0.14048366,\n",
       " 0.009155055,\n",
       " 0.070358,\n",
       " 0.15459217,\n",
       " 0.010453359,\n",
       " 0.0060923374,\n",
       " 0.20410156,\n",
       " 0.0040727886,\n",
       " 0.18506084,\n",
       " 0.173474,\n",
       " 0.009129034,\n",
       " 0.21042842,\n",
       " 0.03036876,\n",
       " 0.09495788,\n",
       " 0.0036917354,\n",
       " 0.015258439,\n",
       " 0.16847633,\n",
       " 0.15996328,\n",
       " 0.17145734,\n",
       " 0.133015,\n",
       " 0.011014573,\n",
       " 0.0040563974,\n",
       " 0.1270824,\n",
       " 0.028144008,\n",
       " 0.0040813964,\n",
       " 0.0023237166,\n",
       " 0.17531788,\n",
       " 0.15593186,\n",
       " 0.046803206,\n",
       " 0.017502805,\n",
       " 0.17289579,\n",
       " 0.16404589,\n",
       " 0.122459054,\n",
       " 0.065479055,\n",
       " 0.020916788,\n",
       " 0.1566747,\n",
       " 0.19049709,\n",
       " 0.14417678,\n",
       " 0.0058531663,\n",
       " 0.23701364,\n",
       " 0.15701665,\n",
       " 0.11028168,\n",
       " 0.17249691,\n",
       " 0.03504165,\n",
       " 0.01854097,\n",
       " 0.16898571,\n",
       " 0.18055896,\n",
       " 0.011602341,\n",
       " 0.027047858,\n",
       " 0.012616121,\n",
       " 0.005583585,\n",
       " 0.16504821,\n",
       " 0.029397927,\n",
       " 0.0106926365,\n",
       " 0.026107548,\n",
       " 0.17099476,\n",
       " 0.22065751,\n",
       " 0.23130345,\n",
       " 0.039219,\n",
       " 0.033272058,\n",
       " 0.19800766,\n",
       " 0.0036425132,\n",
       " 0.007403096,\n",
       " 0.19011158,\n",
       " 0.09381592,\n",
       " 0.2197079,\n",
       " 0.019249199,\n",
       " 0.12456602,\n",
       " 0.003251816,\n",
       " 0.08856591,\n",
       " 0.16918735,\n",
       " 0.019816043,\n",
       " 0.19766599,\n",
       " 0.062664144,\n",
       " 0.0040466464,\n",
       " 0.004965033,\n",
       " 0.035922788,\n",
       " 0.0561385,\n",
       " 0.21349704,\n",
       " 0.06964315,\n",
       " 0.19723977,\n",
       " 0.0036699304,\n",
       " 0.009693074,\n",
       " 0.017199604,\n",
       " 0.21748412,\n",
       " 0.017656256,\n",
       " 0.0055343607,\n",
       " 0.016580084,\n",
       " 0.19163257,\n",
       " 0.103870966,\n",
       " 0.0069642845,\n",
       " 0.0021944232,\n",
       " 0.18725146,\n",
       " 0.012378099,\n",
       " 0.20412552,\n",
       " 0.0028421867,\n",
       " 0.04689686,\n",
       " 0.18452878,\n",
       " 0.022024617,\n",
       " 0.09277155,\n",
       " 0.12739904,\n",
       " 0.2323407,\n",
       " 0.023072608,\n",
       " 0.20602451,\n",
       " 0.008300031,\n",
       " 0.14060457,\n",
       " 0.11063549,\n",
       " 0.0017139199,\n",
       " 0.013963898,\n",
       " 0.012368708,\n",
       " 0.019514645,\n",
       " 0.10637202,\n",
       " 0.0067765336,\n",
       " 0.17776828,\n",
       " 0.002946654,\n",
       " 0.085467905,\n",
       " 0.113375425,\n",
       " 0.1330866,\n",
       " 0.1909449,\n",
       " 0.031324442,\n",
       " 0.011142234,\n",
       " 0.060074523,\n",
       " 0.003127699,\n",
       " 0.014040864,\n",
       " 0.008002297,\n",
       " 0.18473436,\n",
       " 0.2595503,\n",
       " 0.031225545,\n",
       " 0.0575525,\n",
       " 0.01626719,\n",
       " 0.007076287,\n",
       " 0.018601684,\n",
       " 0.08193334,\n",
       " 0.11111361,\n",
       " 0.1817081,\n",
       " 0.06901863,\n",
       " 0.13887061,\n",
       " 0.1272248,\n",
       " 0.002283939,\n",
       " 0.23078561,\n",
       " 0.18238127,\n",
       " 0.12997134,\n",
       " 0.16765727,\n",
       " 0.1880403,\n",
       " 0.0123333065,\n",
       " 0.030817278,\n",
       " 0.00392528,\n",
       " 0.14387393,\n",
       " 0.05575369,\n",
       " 0.002905868,\n",
       " 0.20544457,\n",
       " 0.089677,\n",
       " 0.028201222,\n",
       " 0.15632261,\n",
       " 0.0028097078,\n",
       " 0.0055315583,\n",
       " 0.023605736,\n",
       " 0.1634293,\n",
       " 0.01798537,\n",
       " 0.010079859,\n",
       " 0.24467061,\n",
       " 0.0038220237,\n",
       " 0.21255048,\n",
       " 0.004425703,\n",
       " 0.20675057,\n",
       " 0.015045953,\n",
       " 0.1392756,\n",
       " 0.010280934,\n",
       " 0.19222094,\n",
       " 0.033333797,\n",
       " 0.009899945,\n",
       " 0.0068425634,\n",
       " 0.0064709852,\n",
       " 0.022367397,\n",
       " 0.16025905,\n",
       " 0.061051782,\n",
       " 0.12348637,\n",
       " 0.2238747,\n",
       " 0.08719709,\n",
       " 0.0047973734,\n",
       " 0.019854268,\n",
       " 0.027627468,\n",
       " 0.010181786,\n",
       " 0.18430167,\n",
       " 0.026239919,\n",
       " 0.0069685658,\n",
       " 0.002964643,\n",
       " 0.21276693,\n",
       " 0.15057123,\n",
       " 0.069387846,\n",
       " 0.19214521,\n",
       " 0.011225577,\n",
       " 0.16515951,\n",
       " 0.18224536,\n",
       " 0.19968186,\n",
       " 0.0021388133,\n",
       " 0.026062878,\n",
       " 0.1397749,\n",
       " 0.04464877,\n",
       " 0.19689947,\n",
       " 0.098909885,\n",
       " 0.0035785122,\n",
       " 0.1482468,\n",
       " 0.2865546,\n",
       " 0.17001839,\n",
       " 0.019775493,\n",
       " 0.009786534,\n",
       " 0.10200987,\n",
       " 0.047310892,\n",
       " 0.009022719,\n",
       " 0.12994038,\n",
       " 0.005714752,\n",
       " 0.111698575,\n",
       " 0.114724554,\n",
       " 0.008537352,\n",
       " 0.032120716,\n",
       " 0.0063494593,\n",
       " 0.08052323,\n",
       " 0.1613897,\n",
       " 0.035294186,\n",
       " 0.23144467,\n",
       " 0.18730746,\n",
       " 0.022761883,\n",
       " 0.16069633,\n",
       " 0.20817025,\n",
       " 0.16173184,\n",
       " 0.0068233446,\n",
       " 0.02297731,\n",
       " 0.0048217224,\n",
       " 0.15107892,\n",
       " 0.12688935,\n",
       " 0.21463282,\n",
       " 0.026681326,\n",
       " 0.044746604,\n",
       " 0.18909562,\n",
       " 0.01983037,\n",
       " 0.019224776,\n",
       " 0.004236126,\n",
       " 0.0032588362,\n",
       " 0.19625121,\n",
       " 0.009869382,\n",
       " 0.004375433,\n",
       " 0.097248524,\n",
       " 0.037015516,\n",
       " 0.0043594968,\n",
       " 0.009173988,\n",
       " 0.19149743,\n",
       " 0.013651638,\n",
       " 0.07003459,\n",
       " 0.071729004,\n",
       " 0.004440828,\n",
       " 0.0057831774,\n",
       " 0.03260065,\n",
       " 0.16960613,\n",
       " 0.096229516,\n",
       " 0.14077295,\n",
       " 0.17251937,\n",
       " 0.1907736,\n",
       " 0.01158429,\n",
       " 0.16362773,\n",
       " 0.036444534,\n",
       " 0.017236939,\n",
       " 0.0664039,\n",
       " 0.20314908,\n",
       " 0.22008765,\n",
       " 0.20984906,\n",
       " 0.0062840865,\n",
       " 0.2290193,\n",
       " 0.054618865,\n",
       " 0.06068516,\n",
       " 0.02371799,\n",
       " 0.006377581,\n",
       " 0.2298695,\n",
       " 0.20239373,\n",
       " 0.004501757,\n",
       " 0.14140397,\n",
       " 0.2053328,\n",
       " 0.0102093145,\n",
       " 0.018887935,\n",
       " 0.0036308446,\n",
       " 0.038417514,\n",
       " 0.0031762111,\n",
       " 0.19293769,\n",
       " 0.23955719,\n",
       " 0.21415031,\n",
       " 0.17418228,\n",
       " 0.0024454545,\n",
       " 0.003835059,\n",
       " 0.16863275,\n",
       " 0.019391188,\n",
       " 0.18936844,\n",
       " 0.16851766,\n",
       " 0.017103799,\n",
       " 0.20438801,\n",
       " 0.2356837,\n",
       " 0.0064955386,\n",
       " 0.1761807,\n",
       " 0.0094003985,\n",
       " 0.004793892,\n",
       " 0.22499292,\n",
       " 0.030814715,\n",
       " 0.18129289,\n",
       " 0.0019902822,\n",
       " 0.03786766,\n",
       " 0.030438056,\n",
       " 0.032337237,\n",
       " 0.11288849,\n",
       " 0.0147347255,\n",
       " 0.17378338,\n",
       " 0.04266234,\n",
       " 0.0036865333,\n",
       " 0.04325824,\n",
       " 0.10924164,\n",
       " 0.11163018,\n",
       " 0.17683269,\n",
       " 0.11949957,\n",
       " 0.010359849,\n",
       " 0.002974661,\n",
       " 0.008131713,\n",
       " 0.0044936882,\n",
       " 0.21424675,\n",
       " 0.17166527,\n",
       " 0.025871078,\n",
       " 0.19858222,\n",
       " 0.17599578,\n",
       " 0.16220427,\n",
       " 0.26219305,\n",
       " 0.002855165,\n",
       " 0.08986149,\n",
       " 0.09452193,\n",
       " 0.020260928,\n",
       " 0.031630225,\n",
       " 0.119119346,\n",
       " 0.0043588732,\n",
       " 0.18051325,\n",
       " 0.19328506,\n",
       " 0.14637227,\n",
       " 0.041649256,\n",
       " 0.010668136,\n",
       " 0.056595698,\n",
       " 0.0069149975,\n",
       " 0.19721843,\n",
       " 0.007954399,\n",
       " 0.0065353145,\n",
       " 0.19984789,\n",
       " 0.06784091,\n",
       " 0.019984754,\n",
       " 0.0033861336,\n",
       " 0.03360621,\n",
       " 0.17432922,\n",
       " 0.08556514,\n",
       " 0.004372973,\n",
       " 0.094546884,\n",
       " 0.05234668,\n",
       " 0.11973512,\n",
       " 0.051549267,\n",
       " 0.052076757,\n",
       " 0.0028994444,\n",
       " 0.0016752393,\n",
       " 0.19274183,\n",
       " 0.010568526,\n",
       " 0.06968438,\n",
       " 0.23250975,\n",
       " 0.1540206,\n",
       " 0.024618676,\n",
       " 0.13625975,\n",
       " 0.0067380294,\n",
       " 0.005353995,\n",
       " 0.17261235,\n",
       " 0.1644819,\n",
       " 0.0044932216,\n",
       " 0.017179184,\n",
       " 0.003770253,\n",
       " 0.0030457852,\n",
       " 0.18191506,\n",
       " 0.0061413986,\n",
       " 0.003487257,\n",
       " 0.13848506,\n",
       " 0.014117225,\n",
       " 0.18861394,\n",
       " 0.17913897,\n",
       " 0.17162861,\n",
       " 0.03941575,\n",
       " 0.17931867,\n",
       " 0.018216101,\n",
       " 0.010287581,\n",
       " 0.004776927,\n",
       " 0.10829014,\n",
       " 0.0054392465,\n",
       " 0.0039001154,\n",
       " 0.062196422,\n",
       " 0.011319291,\n",
       " 0.21772444,\n",
       " 0.0032464908,\n",
       " 0.16157614,\n",
       " 0.005682453,\n",
       " 0.1800865,\n",
       " 0.006737625,\n",
       " 0.0093620615,\n",
       " 0.18267266,\n",
       " 0.003481557,\n",
       " 0.020022916,\n",
       " 0.0048156017,\n",
       " 0.036811616,\n",
       " 0.15412407,\n",
       " 0.12867336,\n",
       " 0.0042540138,\n",
       " 0.00530252,\n",
       " 0.0022414757,\n",
       " 0.16339757,\n",
       " 0.046776634,\n",
       " 0.14977434,\n",
       " 0.1014457,\n",
       " 0.057304185,\n",
       " 0.16246712,\n",
       " 0.17863284,\n",
       " 0.18411954,\n",
       " 0.20990764,\n",
       " 0.01692383,\n",
       " 0.20663524,\n",
       " 0.089393206,\n",
       " 0.08453093,\n",
       " 0.17800963,\n",
       " 0.032048404,\n",
       " 0.18614787,\n",
       " 0.16691338,\n",
       " 0.0028336549,\n",
       " 0.0031441634,\n",
       " 0.0126371505,\n",
       " 0.19817112,\n",
       " 0.18080099,\n",
       " 0.172561,\n",
       " 0.20514591,\n",
       " 0.15658668,\n",
       " 0.009331103,\n",
       " 0.15342812,\n",
       " 0.01786167,\n",
       " 0.16128334,\n",
       " 0.21519373,\n",
       " 0.071457125,\n",
       " 0.0019727324,\n",
       " 0.005218163,\n",
       " 0.004615727,\n",
       " 0.022748118,\n",
       " 0.26080805,\n",
       " 0.020190317,\n",
       " 0.21058756,\n",
       " 0.0041213073,\n",
       " 0.0902525,\n",
       " 0.17461334,\n",
       " 0.07467056,\n",
       " 0.20225249,\n",
       " 0.003714157,\n",
       " 0.00474282,\n",
       " 0.19031267,\n",
       " 0.01712818,\n",
       " 0.092957236,\n",
       " 0.21022522,\n",
       " 0.089547396,\n",
       " 0.002741433,\n",
       " 0.006733445,\n",
       " 0.15196772,\n",
       " 0.015478344,\n",
       " 0.114560366,\n",
       " 0.025230253,\n",
       " 0.2000076,\n",
       " 0.0030129624,\n",
       " 0.18403761,\n",
       " 0.14046535,\n",
       " 0.15318893,\n",
       " 0.0118215745,\n",
       " 0.00594425,\n",
       " 0.20176888,\n",
       " 0.0796546,\n",
       " 0.031117668,\n",
       " 0.013538711,\n",
       " 0.0027866384,\n",
       " 0.0023048858,\n",
       " 0.0044511897,\n",
       " 0.008119018,\n",
       " 0.026214046,\n",
       " 0.20824377,\n",
       " 0.00464575,\n",
       " 0.0026174057,\n",
       " 0.17300622,\n",
       " 0.0024793996,\n",
       " 0.07960615,\n",
       " 0.19732545,\n",
       " 0.14898397,\n",
       " 0.038213737,\n",
       " 0.0038837772,\n",
       " 0.05969474,\n",
       " 0.15700027,\n",
       " 0.0062000654,\n",
       " 0.004989076,\n",
       " 0.24029465,\n",
       " 0.047489047,\n",
       " 0.15838549,\n",
       " 0.13345426,\n",
       " 0.14021392,\n",
       " 0.1766544,\n",
       " 0.016779661,\n",
       " 0.1662268,\n",
       " 0.21638793,\n",
       " 0.21447034,\n",
       " 0.10216876,\n",
       " 0.010454257,\n",
       " 0.04035307,\n",
       " 0.0072855656,\n",
       " 0.007881728,\n",
       " 0.0033544835,\n",
       " 0.0127658835,\n",
       " 0.01584599,\n",
       " 0.005012692,\n",
       " 0.002092758,\n",
       " 0.00876107,\n",
       " 0.005163621,\n",
       " 0.008866633,\n",
       " 0.20383441,\n",
       " 0.018881554,\n",
       " 0.06400716,\n",
       " 0.0024626437,\n",
       " 0.20130378,\n",
       " 0.0653973,\n",
       " 0.1834513,\n",
       " 0.10770619,\n",
       " 0.19749922,\n",
       " 0.0023525392,\n",
       " 0.0052215196,\n",
       " 0.0024487425,\n",
       " 0.18489832,\n",
       " 0.08612453,\n",
       " 0.06494449,\n",
       " 0.14777611,\n",
       " 0.1843185,\n",
       " 0.015185895,\n",
       " 0.19557631,\n",
       " 0.011844314,\n",
       " 0.010724784,\n",
       " 0.030987507,\n",
       " 0.100013316,\n",
       " 0.044562608,\n",
       " 0.0028517598,\n",
       " 0.0052231513,\n",
       " 0.05196698,\n",
       " 0.18137908,\n",
       " 0.04400139,\n",
       " 0.0037715172,\n",
       " 0.022759201,\n",
       " 0.064144365,\n",
       " 0.004510397,\n",
       " 0.19300924,\n",
       " 0.016915642,\n",
       " 0.19968349,\n",
       " 0.10321396,\n",
       " 0.19336264,\n",
       " 0.05044554,\n",
       " 0.0036448773,\n",
       " 0.25120077,\n",
       " 0.0045133946,\n",
       " 0.17468177,\n",
       " 0.09042267,\n",
       " 0.047034666,\n",
       " 0.16690119,\n",
       " 0.230536,\n",
       " 0.18802796,\n",
       " 0.002354941,\n",
       " 0.009832919,\n",
       " 0.18738832,\n",
       " 0.011585884,\n",
       " 0.0037299355,\n",
       " 0.014445782,\n",
       " 0.010562546,\n",
       " 0.18781853,\n",
       " 0.06280292,\n",
       " 0.006955217,\n",
       " 0.01743038,\n",
       " 0.17586754,\n",
       " 0.005732272,\n",
       " 0.012208392,\n",
       " 0.008174317,\n",
       " 0.017036999,\n",
       " 0.0017267232,\n",
       " 0.008392137,\n",
       " 0.0032770478,\n",
       " 0.03662539,\n",
       " 0.044770274,\n",
       " 0.019903045,\n",
       " 0.013300915,\n",
       " 0.14232783,\n",
       " 0.005556056,\n",
       " 0.0022678825,\n",
       " 0.16875839,\n",
       " 0.17153876,\n",
       " 0.022517232,\n",
       " 0.0029339672,\n",
       " 0.028832091,\n",
       " 0.0044742725,\n",
       " 0.21937166,\n",
       " 0.12203624,\n",
       " 0.19753377,\n",
       " 0.03602169,\n",
       " 0.20356463,\n",
       " 0.018728245,\n",
       " 0.21796022,\n",
       " 0.18429823,\n",
       " 0.063383415,\n",
       " 0.008617066,\n",
       " 0.011446449,\n",
       " 0.20161247,\n",
       " 0.21644378,\n",
       " 0.0747105,\n",
       " 0.0041536307,\n",
       " 0.079102695,\n",
       " 0.16475447,\n",
       " 0.004492586,\n",
       " 0.21726672,\n",
       " 0.18912403,\n",
       " 0.0030519702,\n",
       " 0.14184198,\n",
       " 0.0058776173,\n",
       " 0.15884684,\n",
       " 0.052062932,\n",
       " 0.0045194332,\n",
       " 0.008322202,\n",
       " 0.0048243743,\n",
       " 0.077767424,\n",
       " 0.021943592,\n",
       " 0.007161863,\n",
       " 0.0070586093,\n",
       " 0.2313369,\n",
       " 0.008557989,\n",
       " 0.023572959,\n",
       " 0.041505653,\n",
       " 0.18344253,\n",
       " 0.032207415,\n",
       " 0.004532047,\n",
       " 0.16014175,\n",
       " 0.010860841,\n",
       " 0.0032115758,\n",
       " 0.11683047,\n",
       " 0.0032995064,\n",
       " 0.0030345423,\n",
       " 0.003166246,\n",
       " 0.0051751984,\n",
       " 0.004454851,\n",
       " 0.18998276,\n",
       " 0.15773724,\n",
       " 0.07466947,\n",
       " 0.0038782815,\n",
       " 0.20011179,\n",
       " 0.06444422,\n",
       " 0.012235176,\n",
       " 0.0035587887,\n",
       " 0.16238171,\n",
       " 0.17760001,\n",
       " 0.17956062,\n",
       " 0.14086021,\n",
       " 0.00434989,\n",
       " 0.18186946,\n",
       " 0.16663368,\n",
       " 0.050514594,\n",
       " 0.16344331,\n",
       " 0.053501427,\n",
       " 0.0061641918,\n",
       " 0.15155767,\n",
       " 0.016197395,\n",
       " 0.15499575,\n",
       " 0.021445744,\n",
       " 0.054311395,\n",
       " 0.1671713,\n",
       " 0.0059246286,\n",
       " 0.0021999842,\n",
       " 0.0028159574,\n",
       " 0.035027593,\n",
       " 0.037415735,\n",
       " 0.011232905,\n",
       " 0.20927668,\n",
       " 0.1180739,\n",
       " 0.07050637,\n",
       " 0.021027423,\n",
       " 0.20621671,\n",
       " 0.2428786,\n",
       " 0.0025463903,\n",
       " 0.19533814,\n",
       " 0.021945285,\n",
       " 0.17104948,\n",
       " 0.112873204,\n",
       " 0.04247125,\n",
       " 0.005348833,\n",
       " 0.09735652,\n",
       " 0.040992603,\n",
       " 0.0052762646,\n",
       " 0.03767227,\n",
       " 0.11754165,\n",
       " 0.058381286,\n",
       " 0.031712387,\n",
       " 0.10577444,\n",
       " 0.06742864,\n",
       " 0.060889173,\n",
       " 0.008483617,\n",
       " 0.019832743,\n",
       " 0.035948627,\n",
       " 0.22039394,\n",
       " 0.003702056,\n",
       " 0.14097576,\n",
       " 0.18423547,\n",
       " 0.23517871,\n",
       " 0.012858336,\n",
       " 0.061154056,\n",
       " 0.14368193,\n",
       " 0.023265226,\n",
       " 0.007472767,\n",
       " 0.022207698,\n",
       " 0.040119298,\n",
       " 0.11930712,\n",
       " 0.17322217,\n",
       " 0.00780555,\n",
       " 0.02065087,\n",
       " 0.19577761,\n",
       " 0.118852876,\n",
       " 0.09193432,\n",
       " 0.0034765184,\n",
       " 0.018322216,\n",
       " 0.009353758,\n",
       " 0.015752556,\n",
       " 0.22641869,\n",
       " 0.11465016,\n",
       " 0.015250039,\n",
       " 0.005740084,\n",
       " 0.23680054,\n",
       " 0.090110004,\n",
       " 0.003750637,\n",
       " 0.006377148,\n",
       " 0.15698643,\n",
       " 0.0080413325,\n",
       " 0.16182835,\n",
       " 0.104468636,\n",
       " 0.010491836,\n",
       " 0.0041010664,\n",
       " 0.2162324,\n",
       " 0.20182979,\n",
       " 0.024852589,\n",
       " 0.2119112,\n",
       " 0.08391415,\n",
       " 0.0032289177,\n",
       " 0.16829313,\n",
       " 0.23306362,\n",
       " 0.0028814634,\n",
       " 0.042981356,\n",
       " 0.0033148157,\n",
       " 0.22513948,\n",
       " 0.022256734,\n",
       " 0.17263196,\n",
       " 0.014652054,\n",
       " 0.15770923,\n",
       " 0.07056871,\n",
       " 0.1974775,\n",
       " 0.016695937,\n",
       " 0.018010603,\n",
       " 0.1956619,\n",
       " 0.1360298,\n",
       " 0.23961182,\n",
       " 0.08717928,\n",
       " 0.057012405,\n",
       " 0.03108109,\n",
       " 0.002516741,\n",
       " 0.004892322,\n",
       " 0.18285675,\n",
       " 0.17189395,\n",
       " 0.016263517,\n",
       " 0.027830517,\n",
       " 0.0074644503,\n",
       " 0.027047217,\n",
       " 0.048065636,\n",
       " 0.003090631,\n",
       " 0.005186619,\n",
       " 0.14313783,\n",
       " 0.15774335,\n",
       " 0.0033544858,\n",
       " 0.22000392,\n",
       " 0.00825575,\n",
       " 0.010455281,\n",
       " 0.17659175,\n",
       " 0.14187604,\n",
       " 0.1861269,\n",
       " 0.23630561,\n",
       " 0.032976896,\n",
       " 0.042182755,\n",
       " 0.0056015085,\n",
       " 0.008987167,\n",
       " 0.0055057704,\n",
       " 0.21884839,\n",
       " 0.18144202,\n",
       " 0.008244294,\n",
       " 0.0031837423,\n",
       " 0.020840382,\n",
       " 0.17616338,\n",
       " 0.0080393925,\n",
       " 0.004356355,\n",
       " 0.1309518,\n",
       " 0.006689913,\n",
       " 0.1805041,\n",
       " 0.07418694,\n",
       " 0.1825955,\n",
       " 0.020760914,\n",
       " 0.20584846,\n",
       " 0.17451186,\n",
       " 0.09805244,\n",
       " 0.0051901843,\n",
       " 0.010125756,\n",
       " 0.20789222,\n",
       " 0.19920892,\n",
       " 0.059377242,\n",
       " 0.014375649,\n",
       " 0.011168987,\n",
       " 0.18853666,\n",
       " 0.21627666,\n",
       " 0.002210126,\n",
       " 0.20781882,\n",
       " 0.0040171966,\n",
       " 0.008919458,\n",
       " 0.19342615,\n",
       " 0.19579417,\n",
       " 0.006194759,\n",
       " 0.14385414,\n",
       " 0.08098396,\n",
       " 0.006040757,\n",
       " 0.13579206,\n",
       " 0.06561444,\n",
       " 0.012001802,\n",
       " 0.0049100206,\n",
       " 0.039130148,\n",
       " 0.19866472,\n",
       " 0.05808057,\n",
       " 0.16151853,\n",
       " 0.0039909906,\n",
       " 0.16532609,\n",
       " 0.013678204,\n",
       " 0.0088021625,\n",
       " 0.19297051,\n",
       " 0.01142482,\n",
       " 0.12643534,\n",
       " 0.17189367,\n",
       " 0.17623521,\n",
       " 0.00515114,\n",
       " 0.13363178,\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023417443"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((x_numpy[0] - x_hat[0])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.753e+03, 1.225e+03, 5.200e+01, 2.000e+00, 2.000e+00, 2.000e+00,\n",
       "        1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
       " array([0.00158216, 0.02318895, 0.04479573, 0.06640252, 0.08800931,\n",
       "        0.10961609, 0.13122287, 0.15282966, 0.17443644, 0.19604322,\n",
       "        0.21765001], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "axs[0].hist(mse_generated)\n",
    "axs[1].hist(mse_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38fe08f3d26a479ec4b53fd1eed47e85ad3eb35a4419302a8faf528c63e8efb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
